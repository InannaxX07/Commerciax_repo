{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvdZ9UokjQPqjdPCO29j3H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InannaxX07/Commerciax_repo/blob/main/sd_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers datasets bitsandbytes accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sMy2h_4trrB",
        "outputId": "1413bd4b-8b40-460d-9677-b663509d157f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting diffusers\n",
            "  Downloading diffusers-0.29.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.23.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from diffusers)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.19.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, diffusers, datasets, bitsandbytes, accelerate\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.32.1 bitsandbytes-0.43.1 datasets-2.20.0 diffusers-0.29.2 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script aims at fine-tuning a Stable Diffusion model on the given dataset\n",
        "# uses excerpts from diffusers example train_text_to_image.py\n",
        "# to know more about SD, read this explainer-\n",
        "# https://poloclub.github.io/diffusion-explainer/\n",
        "\n",
        "\n",
        "import math\n",
        "import logging\n",
        "import accelerate\n",
        "import datasets\n",
        "from accelerate.logging import get_logger\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "import transformers\n",
        "from torchvision import transforms\n",
        "from accelerate import Accelerator\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "import diffusers\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.training_utils import EMAModel, compute_dream_and_update_latents, compute_snr\n",
        "from diffusers.utils import check_min_version, deprecate, is_wandb_available, make_image_grid\n",
        "from diffusers.utils.torch_utils import is_compiled_module\n",
        "import bitsandbytes as bnb\n",
        "import os\n",
        "import random\n",
        "\n",
        "logger = get_logger(__name__, log_level=\"INFO\")\n",
        "\n",
        "def main(\n",
        "        pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\", # name of the model on huggingface\n",
        "        use_8bit_adam=True, # quantised optimizer to save space\n",
        "        dataset_name=\"Babypotatotang/logo-splitted\", # name of dataset on HF\n",
        "        train_data_dir=None, # in case using custom dataset, set this value\n",
        "        resolution=(256,256), # resolution of output image\n",
        "        train_batch_size=2, # batch-size\n",
        "        dataloader_num_workers=20, # number of parallel jobs to run while loading dataset (depends upon num_threads present)\n",
        "        gradient_accumulation_steps=1, # in case of low memory, set this value to aggregate forward pass from multiple steps\n",
        "        num_train_epochs=2, # num epochs\n",
        "        checkpointing_steps=500, # save model chkpt after N steps\n",
        "        output_dir=\"./\" # dir to save the model to\n",
        "        ):\n",
        "\n",
        "    # instantiate accelerate object\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=\"bf16\", # used mixed precision to save space\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps\n",
        "    )\n",
        "\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    logger.info(accelerator.state, main_process_only=False)\n",
        "    if accelerator.is_local_main_process:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_warning()\n",
        "        diffusers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "        diffusers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # Load scheduler, tokenizer and models.\n",
        "    # this section loads multiple parts of Stable Diffusion, read explainer\n",
        "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\n",
        "        pretrained_model_name_or_path, subfolder=\"tokenizer\"\n",
        "    )\n",
        "\n",
        "    text_encoder = CLIPTextModel.from_pretrained(\n",
        "        pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
        "    )\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        pretrained_model_name_or_path, subfolder=\"vae\"\n",
        "    )\n",
        "\n",
        "    unet = UNet2DConditionModel.from_pretrained(\n",
        "        pretrained_model_name_or_path, subfolder=\"unet\"\n",
        "    )\n",
        "\n",
        "    # Freeze vae and text_encoder and set unet to trainable\n",
        "    vae.requires_grad_(False)\n",
        "    text_encoder.requires_grad_(False)\n",
        "    unet.train()\n",
        "\n",
        "    # Initialize the optimizer\n",
        "    if use_8bit_adam:\n",
        "        optimizer_cls = bnb.optim.AdamW8bit\n",
        "    else:\n",
        "        optimizer_cls = torch.optim.AdamW\n",
        "\n",
        "    # using default parameters\n",
        "    optimizer = optimizer_cls(\n",
        "        unet.parameters(),\n",
        "    )\n",
        "\n",
        "    if dataset_name is not None:\n",
        "        # Downloading and loading a dataset from the hub.\n",
        "        dataset = load_dataset(\n",
        "            dataset_name,\n",
        "        )\n",
        "    else:\n",
        "        data_files = {}\n",
        "        if train_data_dir is not None:\n",
        "            data_files[\"train\"] = os.path.join(train_data_dir, \"**\")\n",
        "        dataset = load_dataset(\n",
        "            \"imagefolder\",\n",
        "            data_files=data_files,\n",
        "        )\n",
        "        # See more about loading custom images at\n",
        "        # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize inputs and targets.\n",
        "    column_names = dataset[\"train\"].column_names\n",
        "\n",
        "    # Get the column names for input/target.\n",
        "    # will depend upon the dataset\n",
        "    dataset_columns = (\"image\",\"text\")\n",
        "\n",
        "    image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
        "    caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize input captions and transform the images.\n",
        "    def tokenize_captions(examples, is_train=True):\n",
        "        captions = []\n",
        "        for caption in examples[caption_column]:\n",
        "            if isinstance(caption, str):\n",
        "                captions.append(caption)\n",
        "            elif isinstance(caption, (list, np.ndarray)):\n",
        "                # take a random caption if there are multiple\n",
        "                captions.append(random.choice(caption) if is_train else caption[0])\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
        "                )\n",
        "        inputs = tokenizer(\n",
        "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        return inputs.input_ids\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    train_transforms = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            # transforms.CenterCrop(resolution) if center_crop else transforms.RandomCrop(resolution),\n",
        "            # transforms.RandomHorizontalFlip() if random_flip else transforms.Lambda(lambda x: x),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    def preprocess_train(examples):\n",
        "        images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
        "        examples[\"image\"] = [train_transforms(image) for image in images]\n",
        "        examples[\"text\"] = tokenize_captions(examples)\n",
        "        return examples\n",
        "\n",
        "    with accelerator.main_process_first():\n",
        "        # Set the training transforms\n",
        "        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        image = torch.Tensor(examples[0]['image'])\n",
        "        image = torch.stack([example[\"image\"] for example in examples])\n",
        "        image = image.to(memory_format=torch.contiguous_format).float()\n",
        "        text = torch.stack([example[\"text\"] for example in examples])\n",
        "        return {\"pixel_values\": image, \"text\": text}\n",
        "\n",
        "    # DataLoaders creation:\n",
        "    # takes care of preprocessing and tokenization\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        batch_size=train_batch_size,\n",
        "        num_workers=dataloader_num_workers,\n",
        "    )\n",
        "\n",
        "    # Scheduler and math around the number of training steps.\n",
        "    # what does this do?? haha\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
        "\n",
        "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "    # get learning rate scheduler\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"constant\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=500 * accelerator.num_processes,\n",
        "        num_training_steps=max_train_steps * accelerator.num_processes,\n",
        "    )\n",
        "\n",
        "    # Prepare everything with our `accelerator`.\n",
        "    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        unet, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    # For mixed precision training we cast all non-trainable weights (vae, non-lora text_encoder and non-lora unet) to half-precision\n",
        "    # as these weights are only used for inference, keeping weights in full precision is not required.\n",
        "    weight_dtype = torch.float32\n",
        "    if accelerator.mixed_precision == \"fp16\":\n",
        "        weight_dtype = torch.float16\n",
        "    elif accelerator.mixed_precision == \"bf16\":\n",
        "        weight_dtype = torch.bfloat16\n",
        "\n",
        "    # Move text_encode and vae to gpu and cast to weight_dtype\n",
        "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
        "    vae.to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
        "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "    # Afterwards we recalculate our number of training epochs\n",
        "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    # Function for unwrapping if model was compiled with `torch.compile`.\n",
        "    def unwrap_model(model):\n",
        "        model = accelerator.unwrap_model(model)\n",
        "        model = model._orig_mod if is_compiled_module(model) else model\n",
        "        return model\n",
        "\n",
        "    # Train!\n",
        "    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
        "    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
        "    global_step = 0\n",
        "    first_epoch = 0\n",
        "    initial_global_step = 0\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        range(0, max_train_steps),\n",
        "        initial=initial_global_step,\n",
        "        desc=\"Steps\",\n",
        "        # Only show the progress bar once on each machine.\n",
        "        disable=not accelerator.is_local_main_process,\n",
        "    )\n",
        "\n",
        "    for epoch in range(first_epoch, num_train_epochs):\n",
        "        train_loss = 0.0\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(unet):\n",
        "                # Convert images to latent space\n",
        "                latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample()\n",
        "                latents = latents * vae.config.scaling_factor\n",
        "\n",
        "                # Sample noise that we'll add to the latents\n",
        "                noise = torch.randn_like(latents)\n",
        "\n",
        "                bsz = latents.shape[0]\n",
        "                # Sample a random timestep for each image\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "                timesteps = timesteps.long()\n",
        "\n",
        "                # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                # (this is the forward diffusion process)\n",
        "\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Get the text embedding for conditioning\n",
        "                encoder_hidden_states = text_encoder(batch[\"text\"], return_dict=False)[0]\n",
        "\n",
        "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                    target = noise\n",
        "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "\n",
        "                # Predict the noise residual and compute loss\n",
        "                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
        "\n",
        "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "\n",
        "\n",
        "                # Gather the losses across all processes for logging (if we use distributed training).\n",
        "                avg_loss = accelerator.gather(loss.repeat(train_batch_size)).mean()\n",
        "                train_loss += avg_loss.item() / gradient_accumulation_steps\n",
        "\n",
        "                # Backpropagate\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "            if accelerator.sync_gradients:\n",
        "\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "                accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
        "                train_loss = 0.0\n",
        "\n",
        "                if global_step % checkpointing_steps == 0:\n",
        "                    if accelerator.is_main_process:\n",
        "\n",
        "                        save_path = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
        "                        accelerator.save_state(save_path)\n",
        "                        logger.info(f\"Saved state to {save_path}\")\n",
        "\n",
        "            logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "\n",
        "            if global_step >= max_train_steps:\n",
        "                break\n",
        "\n",
        "    # Create the pipeline using the trained modules and save it.\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "    if accelerator.is_main_process:\n",
        "            unet = unwrap_model(unet)\n",
        "\n",
        "            pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                pretrained_model_name_or_path,\n",
        "                text_encoder=text_encoder,\n",
        "                vae=vae,\n",
        "                unet=unet,\n",
        "            )\n",
        "            pipeline.save_pretrained(output_dir)\n",
        "\n",
        "            # Run a final round of inference.\n",
        "            images = []\n",
        "\n",
        "    accelerator.end_training()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "lFT50wR1M8WD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}